'''
tokenize & save tokenized insns
build vocab
'''
import os
import re
import pickle
from glob import glob
from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor
import numpy as np
import argparse
from AsmVocab import AsmLMVocab

dir_path = os.path.dirname(os.path.realpath(__file__))

parser = argparse.ArgumentParser()
parser.add_argument('-i', '--input_dir', type=str, help='input dir to pkl generated by IDA')
parser.add_argument('-o', '--output_dir', type=str, nargs='?',
                    help='Output dir', default='./preprocess-outputs')
parser.add_argument('-n', '--workers', type=int, nargs='?',
                    help='Max Workers', default=48)
args = parser.parse_args()



def normalize_insn(asm):
    opcode, op_str = asm.split('\t')
    
    op_str = op_str.replace(' + ', '+')
    op_str = op_str.replace(' - ', '-')
    op_str = op_str.replace(' * ', '*')
    op_str = op_str.replace(' : ', ':')
    # op_str = op_str.replace(',', ' ,')
    op_str = re.sub('0x[0-9a-f]+', 'const', op_str)
    # print(f'{opcode} {op_str}')
    if op_str:
        opnd_strs = op_str.split(', ')
    else:
        opnd_strs = []

    return opcode, opnd_strs


def gen_vocab_per_file(pkl_file, output_dir):
    token_vocab = set()
    itype_vocab = set()
    opnd_type_vocab = set()
    reg_id_vocab = set()
    reg_r_vocab = set()
    reg_w_vocab = set()
    eflags_vocab = set()
    
    with open(pkl_file, 'rb') as f:
        load = pickle.load(f)
        for func_info in tqdm(load):
            for insn_addr,disasm,itype,op_info,eflags in func_info:
                if not disasm:
                    continue

                opcode, opnd_strs = normalize_insn(disasm)

                opcode_tokens = opcode.split()
                insn_tokens = opcode_tokens + ' , '.join(opnd_strs).split()

                token_vocab.update(set(insn_tokens))
                itype_vocab.add(str(itype))
                eflags_vocab.add(str(eflags))
                
                for i in range(len(opnd_strs)):
                    opnd_type_vocab.add(str(op_info[i][0]))
                    reg_id_vocab.add(str(op_info[i][1]))
                    reg_r_vocab.add(str(op_info[i][2]))
                    reg_w_vocab.add(str(op_info[i][3]))


    out_name = pkl_file.split(os.path.sep)[-1]
    with open(f'{output_dir}/{out_name}.token_vocab', 'w') as f:
        f.write(' '.join(list(token_vocab)))
    with open(f'{output_dir}/{out_name}.itype_vocab', 'w') as f:
        f.write(' '.join(list(itype_vocab)))
    with open(f'{output_dir}/{out_name}.opnd_type_vocab', 'w') as f:
        f.write(' '.join(list(opnd_type_vocab)))
    with open(f'{output_dir}/{out_name}.reg_id_vocab', 'w') as f:
        f.write(' '.join(list(reg_id_vocab)))
    with open(f'{output_dir}/{out_name}.reg_r_vocab', 'w') as f:
        f.write(' '.join(list(reg_r_vocab)))
    with open(f'{output_dir}/{out_name}.reg_w_vocab', 'w') as f:
        f.write(' '.join(list(reg_w_vocab)))
    with open(f'{output_dir}/{out_name}.eflags_vocab', 'w') as f:
        f.write(' '.join(list(eflags_vocab)))


def merge_vocab(vocab_type):
    vocab = set()
    for vocab_file in tqdm(glob('{}/*.{}'.format(args.output_dir, vocab_type))):
        with open(vocab_file, 'r') as f:
            vocab.update(set(f.read().split()))
    
    print(len(vocab))
    vocab = list(vocab)
    vocab.sort()
    vocab = AsmLMVocab(vocab)
    vocab.save(f'{dir_path}/{vocab_type}.txt')


def gen_vocab(dataset_path):
    os.makedirs(args.output_dir, exist_ok=True)

    with ProcessPoolExecutor(max_workers=args.workers) as executor:
        for pkl_file in tqdm(glob('{}/*.pkl'.format(dataset_path))):
            executor.submit(gen_vocab_per_file, pkl_file, args.output_dir) 

    # vocab = set()
    # for vocab_file in tqdm(glob('{}/*.vocab'.format(args.output_dir))):
    #     with open(vocab_file, 'r') as f:
    #         vocab.update(set(f.read().split()))
    
    # print(len(vocab))
    # vocab = list(vocab)
    # vocab.sort()
    # vocab = AsmLMVocab(vocab)
    # vocab.save(f'{dir_path}/vocab.txt')

    merge_vocab('token_vocab')
    merge_vocab('itype_vocab')
    merge_vocab('opnd_type_vocab')
    merge_vocab('reg_id_vocab')
    merge_vocab('reg_r_vocab')
    merge_vocab('reg_w_vocab')
    merge_vocab('eflags_vocab')



if __name__ == '__main__':
    gen_vocab(args.input_dir)
    os.system(f'rm -rf {args.output_dir}')
    # gen_vocab('dataset/outputs')